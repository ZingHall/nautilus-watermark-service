name: Deploy to Nitro Enclave

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'Containerfile'
      - 'Makefile'
      - 'Cargo.toml'
      - '.github/workflows/deploy-enclave.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      auto_apply:
        description: 'Automatically apply Terraform changes'
        required: false
        default: false
        type: boolean

env:
  AWS_ACCOUNT_ID: 287767576800
  AWS_REGION: ap-northeast-1
  ENCLAVE_APP: zing-watermark
  S3_BUCKET_STAGING: zing-enclave-artifacts-staging
  S3_BUCKET_PRODUCTION: zing-enclave-artifacts-production
  EIF_PATH_STAGING: eif/staging
  EIF_PATH_PRODUCTION: eif/production

jobs:
  deploy:
    name: Deploy to Nitro Enclave
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine environment
        id: env
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            echo "environment=staging" >> $GITHUB_OUTPUT
          fi
          echo "Deploying to: ${{ steps.env.outputs.environment }}"

      - name: Shorten commit hash
        id: shorten-commit
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Set S3 bucket and path
        id: s3
        run: |
          if [ "${{ steps.env.outputs.environment }}" == "production" ]; then
            echo "bucket=${{ env.S3_BUCKET_PRODUCTION }}" >> $GITHUB_OUTPUT
            echo "path=${{ env.EIF_PATH_PRODUCTION }}" >> $GITHUB_OUTPUT
          else
            echo "bucket=${{ env.S3_BUCKET_STAGING }}" >> $GITHUB_OUTPUT
            echo "path=${{ env.EIF_PATH_STAGING }}" >> $GITHUB_OUTPUT
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/github-actions-cicd-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Checkout infrastructure repository
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/zing-infra
          path: zing-infra
          token: ${{ secrets.ZING_INFRA_TOKEN }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
          terraform_wrapper: false

      - name: Force unlock Terraform state (if locked)
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üîì Checking for stuck Terraform state lock..."
          
          # Initialize Terraform with backend config
          terraform init -reconfigure \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=$TF_BACKEND_DYNAMODB_TABLE"
          
          # Try to run a plan to check if state is locked
          set +e  # Don't exit on error
          terraform plan -detailed-exitcode 2>&1 | tee plan_output.txt
          PLAN_EXIT_CODE=$?
          set -e
          
          # Check if the plan failed due to a lock
          if grep -q "Error acquiring the state lock" plan_output.txt; then
            echo "‚ö†Ô∏è Terraform state is locked, attempting to extract lock ID..."
            
            # Extract lock ID from error message
            LOCK_ID=$(grep -oP 'ID:\s+\K[a-f0-9-]+' plan_output.txt | head -1)
            
            if [ -n "$LOCK_ID" ]; then
              echo "Found lock ID: $LOCK_ID"
              echo "Attempting to force unlock..."
              terraform force-unlock -force "$LOCK_ID"
              echo "‚úÖ Successfully unlocked Terraform state"
            else
              echo "‚ùå Could not extract lock ID from error message"
              echo "Lock may need to be manually removed"
              exit 1
            fi
          else
            echo "‚úÖ Terraform state is not locked, proceeding..."
          fi
          
          rm -f plan_output.txt

      - name: Verify Terraform permissions (plan only)
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üîç Verifying Terraform permissions with terraform plan..."
          echo "This step validates that the IAM role has all required permissions"
          echo "before building EIF files or making any changes."
          
          # Run terraform plan to verify permissions
          # This will fail if any required permissions are missing
          # Use current eif_version from variables.tf for this check
          set +e  # Don't exit on error, we'll handle it manually
          terraform plan -detailed-exitcode
          PLAN_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          if [ "$PLAN_EXIT_CODE" = "1" ]; then
            echo "‚ùå terraform plan failed - check IAM permissions"
            echo ""
            echo "The IAM role 'github-actions-cicd-role' is missing required permissions."
            echo ""
            echo "Current policy includes (with wildcards):"
            echo "  - s3:*, dynamodb:*, ec2:*, vpc:*, rds:*"
            echo "  - route53:*, acm:*, elasticloadbalancing:*"
            echo "  - logs:*, cloudwatch:*, autoscaling:*"
            echo "  - Limited IAM permissions (roles, instance profiles)"
            echo ""
            echo "Check the error message above to identify the specific missing permission."
            echo "Then update zing-infra/modules/aws/github-cicd/main.tf to add it."
            echo ""
            echo "If it's a new service, you may need to add a wildcard (e.g., 'servicename:*')"
            echo "or specific permissions to the terraform_policy."
            exit 1
          elif [ "$PLAN_EXIT_CODE" = "2" ]; then
            echo "‚úÖ terraform plan succeeded - changes detected (this is expected)"
            echo "All required permissions are available"
          elif [ "$PLAN_EXIT_CODE" = "0" ]; then
            echo "‚úÖ terraform plan succeeded - no changes needed"
            echo "All required permissions are available"
          else
            echo "‚ö†Ô∏è terraform plan returned unexpected exit code: $PLAN_EXIT_CODE"
            echo "This might indicate permission issues. Please check the output above."
            exit 1
          fi

      - name: Check if EIF already exists in S3
        id: check-eif
        run: |
          EIF_S3_PATH="s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif"
          if aws s3 ls "$EIF_S3_PATH" 2>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ EIF file already exists in S3, skipping build"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è EIF file not found, will build new one"
          fi

      - name: Verify allowed_endpoints.yaml configuration
        if: steps.check-eif.outputs.exists == 'false'
        run: |
          ALLOWLIST_PATH="src/nautilus-server/src/apps/${{ env.ENCLAVE_APP }}/allowed_endpoints.yaml"
          
          echo "üîç Verifying allowed_endpoints.yaml configuration..."
          echo "Enclave App: ${{ env.ENCLAVE_APP }}"
          echo "Expected path: $ALLOWLIST_PATH"
          
          if [ ! -f "$ALLOWLIST_PATH" ]; then
            echo "‚ùå Error: allowed_endpoints.yaml not found at $ALLOWLIST_PATH"
            echo "This file is required for the enclave to access external endpoints."
            echo "Please create the file with the following structure:"
            echo "---"
            echo "endpoints:"
            echo "  - https://example.com:443"
            exit 1
          fi
          
          echo "‚úÖ allowed_endpoints.yaml found"
          
          # Display endpoints if yq is available, otherwise just show file exists
          if command -v yq >/dev/null 2>&1; then
            echo ""
            echo "üìã Configured endpoints:"
            yq e '.endpoints[]' "$ALLOWLIST_PATH" | sed 's/^/  - /' || echo "  (Unable to parse endpoints)"
          else
            echo "‚ÑπÔ∏è Install yq to display endpoint details: brew install yq"
            echo "File content:"
            cat "$ALLOWLIST_PATH"
          fi
          
          echo ""
          echo "‚úÖ Endpoint configuration verified"

      - name: Install yq
        if: steps.check-eif.outputs.exists == 'false'
        uses: mikefarah/yq@v4.40.5
        with:
          version: '4.40.5'

      - name: Update run.sh with endpoint configuration
        if: steps.check-eif.outputs.exists == 'false'
        run: |
          echo "üîß Updating run.sh with endpoint configuration from allowed_endpoints.yaml..."
          ALLOWLIST_PATH="src/nautilus-server/src/apps/${{ env.ENCLAVE_APP }}/allowed_endpoints.yaml"
          RUN_SH_PATH="src/nautilus-server/run.sh"
          AWS_REGION="${{ env.AWS_REGION }}"
          
          # Read endpoints from allowed_endpoints.yaml
          ENDPOINTS=$(yq e '.endpoints | join(" ")' "$ALLOWLIST_PATH" 2>/dev/null || echo "")
          
          if [ -z "$ENDPOINTS" ]; then
            echo "‚ö†Ô∏è No endpoints found in $ALLOWLIST_PATH, skipping run.sh update"
            exit 0
          fi
          
          echo "üìã Found endpoints (before region patching):"
          echo "$ENDPOINTS" | tr ' ' '\n' | sed 's/^/  - /'
          
          # Apply region patching for AWS services (matching configure_enclave.sh behavior)
          # Replace any existing region in kms.* / secretsmanager.* with the configured AWS_REGION
          if [ -n "$ENDPOINTS" ]; then
            ENDPOINTS=$(echo "$ENDPOINTS" \
              | sed "s|kms\.[^.]*\.amazonaws\.com|kms.$AWS_REGION.amazonaws.com|g" \
              | sed "s|secretsmanager\.[^.]*\.amazonaws\.com|secretsmanager.$AWS_REGION.amazonaws.com|g")
            echo ""
            echo "üìã Endpoints after region patching:"
            echo "$ENDPOINTS" | tr ' ' '\n' | sed 's/^/  - /'
          fi
          
          # Build endpoints configuration (matching configure_enclave.sh behavior)
          # IMPORTANT: Extract hostname from endpoint (remove port numbers)
          # /etc/hosts format: IP_ADDRESS   HOSTNAME (no port numbers allowed)
          ip=64
          tmp_endpoints_config=$(mktemp)
          for ep in $ENDPOINTS; do
            # Extract hostname by removing port number (e.g., "host:443" -> "host")
            hostname=$(echo "$ep" | sed 's/:.*$//')
            echo "  Processing endpoint: $ep -> hostname: $hostname"
            echo "echo \"127.0.0.${ip}   ${hostname}\" >> /etc/hosts" >> "$tmp_endpoints_config"
            ip=$((ip+1))
          done
          
          echo ""
          echo "üìã Generated /etc/hosts configuration:"
          cat "$tmp_endpoints_config"
          
          echo ""
          echo "üìù Updating /etc/hosts configuration in run.sh..."
          
          # Remove any existing endpoint lines (except the first localhost line)
          # This removes lines like: echo "127.0.0.64   hostname" >> /etc/hosts
          sed -i '/echo "127.0.0.[0-9]*   .*" >> \/etc\/hosts/d' "$RUN_SH_PATH"
          
          # Ensure the localhost line exists
          if ! grep -q 'echo "127.0.0.1   localhost" > /etc/hosts' "$RUN_SH_PATH"; then
            sed -i '/# Add a hosts record/a\echo "127.0.0.1   localhost" > /etc/hosts' "$RUN_SH_PATH"
          fi
          
          # Add the new endpoint configuration after the localhost line
          # Use the temporary file we created above
          if [ -s "$tmp_endpoints_config" ]; then
            # Find the line number of the localhost line
            localhost_line=$(grep -n 'echo "127.0.0.1   localhost" > /etc/hosts' "$RUN_SH_PATH" | cut -d: -f1 | head -1)
            
            if [ -z "$localhost_line" ]; then
              echo "‚ùå ERROR: Could not find localhost line in run.sh"
              echo "   File content around expected location:"
              grep -A 5 -B 5 "localhost" "$RUN_SH_PATH" | head -15
              rm "$tmp_endpoints_config"
              exit 1
            fi
            
            echo "   Found localhost line at line $localhost_line"
            
            # Insert endpoint configuration after the localhost line using sed
            # The 'r' command reads the file and inserts it after the matching line
            sed -i "${localhost_line}r $tmp_endpoints_config" "$RUN_SH_PATH"
            
            echo "‚úÖ Inserted endpoint configuration after line $localhost_line"
          else
            echo "‚ö†Ô∏è  WARNING: No endpoint configuration to insert (file is empty)"
          fi
          
          # Clean up temporary file
          rm -f "$tmp_endpoints_config"
          
          echo "‚úÖ Updated /etc/hosts configuration"
          
          # Update traffic forwarder configuration
          # Traffic forwarder bridges TCP/IP (127.0.0.x:443) to VSOCK (CID 3:810x)
          # Each endpoint gets:
          #   - A unique 127.0.0.x IP (mapped in /etc/hosts)
          #   - A unique VSOCK port (8101, 8102, ...)
          # The vsock-proxy on the EC2 host forwards VSOCK traffic to the actual endpoint
          echo ""
          echo "üìù Updating traffic forwarder configuration in run.sh..."
          
          ip_forwarder=64
          port_forwarder=8101
          traffic_config=""
          endpoint_count=0
          for ep in $ENDPOINTS; do
            traffic_config="${traffic_config}python3 /traffic_forwarder.py 127.0.0.${ip_forwarder} 443 3 ${port_forwarder} &"$'\n'
            echo "  Endpoint $((endpoint_count+1)): $ep ‚Üí 127.0.0.${ip_forwarder}:443 ‚Üí VSOCK CID 3:${port_forwarder}"
            ip_forwarder=$((ip_forwarder+1))
            port_forwarder=$((port_forwarder+1))
            endpoint_count=$((endpoint_count+1))
          done
          
          if [ $endpoint_count -eq 0 ]; then
            echo "‚ö†Ô∏è No endpoints to configure traffic forwarders for"
          else
            echo "  Configured $endpoint_count traffic forwarder(s)"
          fi
          
          # Remove any existing traffic forwarder lines
          sed -i '/python3 \/traffic_forwarder.py/d' "$RUN_SH_PATH"
          
          # Add the new traffic forwarder configuration
          tmp_traffic=$(mktemp)
          echo "$traffic_config" > "$tmp_traffic"
          sed -i "/# Traffic-forwarder-block/ r $tmp_traffic" "$RUN_SH_PATH"
          rm "$tmp_traffic"
          
          echo "‚úÖ Updated traffic forwarder configuration"
          
          # Verify consistency between /etc/hosts and traffic forwarder configurations
          echo ""
          echo "üîç Verifying configuration consistency..."
          HOSTS_COUNT=$(grep -c 'echo "127.0.0.[0-9]*' "$RUN_SH_PATH" || echo "0")
          FORWARDER_COUNT=$(grep -c 'traffic_forwarder.py' "$RUN_SH_PATH" || echo "0")
          
          # Subtract 1 from HOSTS_COUNT because it includes the localhost line
          if [ "$HOSTS_COUNT" -gt 0 ]; then
            HOSTS_COUNT=$((HOSTS_COUNT - 1))
          fi
          
          if [ "$HOSTS_COUNT" -eq "$FORWARDER_COUNT" ]; then
            echo "‚úÖ Configuration consistent: $HOSTS_COUNT endpoint(s) in /etc/hosts, $FORWARDER_COUNT traffic forwarder(s)"
          else
            echo "‚ö†Ô∏è Configuration mismatch: $HOSTS_COUNT endpoint(s) in /etc/hosts, $FORWARDER_COUNT traffic forwarder(s)"
            echo "   This may indicate a configuration issue"
          fi
          
          # Verify port 3001 VSOCK listener exists (required by configure_enclave.sh)
          echo ""
          echo "üîç Verifying port 3001 VSOCK listener..."
          if ! grep -q "VSOCK-LISTEN:3001" "$RUN_SH_PATH"; then
            echo "‚ö†Ô∏è Port 3001 VSOCK listener not found, adding it..."
            # Add port 3001 listener after port 3000 listener using a temporary file
            tmp_port3001=$(mktemp)
            printf '\n# Listen on VSOCK Port 3001 and forward to localhost 3001\nsocat VSOCK-LISTEN:3001,reuseaddr,fork TCP:localhost:3001 &\n' > "$tmp_port3001"
            sed -i '/socat VSOCK-LISTEN:3000,reuseaddr,fork TCP:localhost:3000 &/ r '"$tmp_port3001" "$RUN_SH_PATH"
            rm "$tmp_port3001"
            echo "‚úÖ Added port 3001 VSOCK listener"
          else
            echo "‚úÖ Port 3001 VSOCK listener already exists"
          fi
          
          echo ""
          echo "üìÑ Updated run.sh content:"
          echo "--- /etc/hosts section ---"
          grep -A 10 'echo "127.0.0.1   localhost"' "$RUN_SH_PATH" | head -15
          echo ""
          echo "--- Traffic forwarder section ---"
          grep -A 5 '# Traffic-forwarder-block' "$RUN_SH_PATH" | head -10
          echo ""
          echo "--- VSOCK listeners section ---"
          grep -A 2 'VSOCK-LISTEN:300' "$RUN_SH_PATH"
          
          # Verify that endpoints were actually added
          echo ""
          echo "üîç Verifying endpoint configuration was added..."
          HOSTS_LINES=$(grep -c 'echo "127.0.0.[0-9]*   .*" >> /etc/hosts' "$RUN_SH_PATH" || echo "0")
          if [ "$HOSTS_LINES" -eq 0 ]; then
            echo "‚ùå ERROR: No endpoint configuration found in run.sh!"
            echo "   This means endpoints were not added. Check the sed command above."
            exit 1
          else
            echo "‚úÖ Found $HOSTS_LINES endpoint configuration line(s) in run.sh"
          fi

      - name: Build EIF file
        if: steps.check-eif.outputs.exists == 'false'
        run: |
          echo "üî® Building EIF file..."
          echo "Enclave App: ${{ env.ENCLAVE_APP }}"
          echo "This build will include allowed_endpoints.yaml from src/nautilus-server/src/apps/${{ env.ENCLAVE_APP }}/"
          echo ""
          
          make ENCLAVE_APP=${{ env.ENCLAVE_APP }}
          
          if [ ! -f "out/nitro.eif" ]; then
            echo "‚ùå EIF file not found after build"
            exit 1
          fi
          
          echo "‚úÖ EIF file built successfully"
          ls -lh out/nitro.eif
          
          # Verify that the build process completed (Containerfile should have copied allowed_endpoints.yaml)
          echo ""
          echo "‚ÑπÔ∏è Note: allowed_endpoints.yaml should be included in the EIF file"
          echo "The file is copied during Docker build in the Containerfile"

      - name: Upload EIF to S3
        if: steps.check-eif.outputs.exists == 'false'
        id: upload-eif
        run: |
          EIF_S3_PATH="s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif"
          YAML_S3_PATH="s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/allowed_endpoints-${{ steps.shorten-commit.outputs.sha_short }}.yaml"
          ALLOWLIST_PATH="src/nautilus-server/src/apps/${{ env.ENCLAVE_APP }}/allowed_endpoints.yaml"
          
          echo "üì§ Uploading EIF to $EIF_S3_PATH"
          aws s3 cp out/nitro.eif "$EIF_S3_PATH" \
            --metadata "commit=${{ github.sha }},commit_short=${{ steps.shorten-commit.outputs.sha_short }},build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          
          echo "üì§ Uploading allowed_endpoints.yaml to $YAML_S3_PATH"
          aws s3 cp "$ALLOWLIST_PATH" "$YAML_S3_PATH" \
            --metadata "commit=${{ github.sha }},commit_short=${{ steps.shorten-commit.outputs.sha_short }},build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          
          echo "eif_path=$EIF_S3_PATH" >> $GITHUB_OUTPUT
          echo "‚úÖ EIF and allowed_endpoints.yaml uploaded successfully"

      - name: Update Terraform EIF version
        id: update-eif-version
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        run: |
          echo "üìù Updating Terraform configuration..."
          
          # Update eif_version default value in variables.tf using Python
          python3 << 'PYTHON_SCRIPT'
          import re
          
          with open('variables.tf', 'r') as f:
              content = f.read()
          
          # Pattern to match eif_version default value in variables.tf
          pattern = r'(variable\s+"eif_version"\s*\{[^}]*default\s*=\s*)"[^"]*"'
          replacement = r'\1"${{ steps.shorten-commit.outputs.sha_short }}"'
          
          UPDATED_FILE = None
          
          if re.search(pattern, content):
              # Update existing eif_version default value
              content = re.sub(pattern, replacement, content)
              with open('variables.tf', 'w') as f:
                  f.write(content)
              UPDATED_FILE = 'variables.tf'
          else:
              # Fallback: try to find and update in main.tf (for backwards compatibility)
              with open('main.tf', 'r') as f:
                  main_content = f.read()
              
              main_pattern = r'eif_version\s*=\s*"[^"]*"'
              main_replacement = f'eif_version    = "${{ steps.shorten-commit.outputs.sha_short }}"'
              
              if re.search(main_pattern, main_content):
                  main_content = re.sub(main_pattern, main_replacement, main_content)
                  with open('main.tf', 'w') as f:
                      f.write(main_content)
                  UPDATED_FILE = 'main.tf'
          
          if UPDATED_FILE:
              print(f"Updated eif_version in {UPDATED_FILE}")
          else:
              print("ERROR: Could not find eif_version to update")
              exit(1)
          PYTHON_SCRIPT
          
          echo "‚úÖ Updated eif_version default to ${{ steps.shorten-commit.outputs.sha_short }}"
          echo "Changes:"
          git diff variables.tf || git diff main.tf || true
          
          # Determine which file was updated and set as output for next step
          if git diff --name-only | grep -q variables.tf; then
            echo "updated_file=variables.tf" >> $GITHUB_OUTPUT
          elif git diff --name-only | grep -q main.tf; then
            echo "updated_file=main.tf" >> $GITHUB_OUTPUT
          else
            echo "ERROR: No changes detected in variables.tf or main.tf"
            exit 1
          fi

      - name: Commit Terraform changes
        id: commit-changes
        working-directory: zing-infra
        env:
          GITHUB_TOKEN: ${{ secrets.ZING_INFRA_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Determine if we should create a PR or push directly to main
          CREATE_PR=false
          if [ "${{ github.event_name }}" == "push" ] && [ "${{ github.ref }}" == "refs/heads/main" ] && [ "${{ steps.env.outputs.environment }}" == "production" ]; then
            CREATE_PR=true
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ github.event.inputs.auto_apply }}" != "true" ]; then
            CREATE_PR=true
          fi
          
          # Fetch latest changes from remote
          git fetch origin
          
          # Determine the default branch (master or main)
          DEFAULT_BRANCH=$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)
          echo "Default branch is: $DEFAULT_BRANCH"
          
          # Create branch if we need a PR, otherwise work on default branch
          if [ "$CREATE_PR" == "true" ]; then
            BRANCH_NAME="update-enclave-eif-${{ steps.shorten-commit.outputs.sha_short }}"
            git checkout -b "$BRANCH_NAME" "origin/$DEFAULT_BRANCH"
            echo "Created branch: $BRANCH_NAME"
          else
            git checkout "$DEFAULT_BRANCH" || git checkout -b "$DEFAULT_BRANCH"
            git pull origin "$DEFAULT_BRANCH" --rebase || true
            echo "Working on $DEFAULT_BRANCH branch"
          fi
          
          # Get the updated file from previous step
          UPDATED_FILE="${{ steps.update-eif-version.outputs.updated_file }}"
          if [ -z "$UPDATED_FILE" ]; then
            echo "ERROR: Could not determine which file was updated"
            exit 1
          fi
          
          echo "Updating file: $UPDATED_FILE"
          
          git add "environments/${{ steps.env.outputs.environment }}/nautilus-enclave/$UPDATED_FILE"
          
          # Check if there are any changes to commit
          if git diff --staged --quiet; then
            echo "‚ö†Ô∏è No changes to commit (file may already be up to date)"
            echo "skip_commit=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "skip_commit=false" >> $GITHUB_OUTPUT
          
          COMMIT_MSG="chore: update enclave EIF version to ${{ steps.shorten-commit.outputs.sha_short }}

          - EIF file: s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif
          - Source commit: ${{ github.sha }}
          - Environment: ${{ steps.env.outputs.environment }}"
          
          git commit -m "$COMMIT_MSG"
          echo "‚úÖ Committed changes to $UPDATED_FILE"
          
          # Push the branch
          if [ "$CREATE_PR" == "true" ]; then
            git push origin "$BRANCH_NAME"
            echo "Pushed branch: $BRANCH_NAME"
          else
            git push origin "$DEFAULT_BRANCH"
            echo "Pushed to $DEFAULT_BRANCH branch"
          fi

      - name: Create Pull Request for Terraform changes
        if: |
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'production') ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply != 'true')
        working-directory: zing-infra
        env:
          GITHUB_TOKEN: ${{ secrets.ZING_INFRA_TOKEN }}
        run: |
          # Skip if commit was skipped
          if [ "${{ steps.commit-changes.outputs.skip_commit }}" == "true" ]; then
            echo "‚ö†Ô∏è Skipping PR creation (no changes to commit)"
            exit 0
          fi
          
          BRANCH_NAME="update-enclave-eif-${{ steps.shorten-commit.outputs.sha_short }}"
          
          # Check if PR already exists
          PR_EXISTS=$(gh pr list --head "$BRANCH_NAME" --json number --jq '.[0].number' 2>/dev/null || echo "")
          
          if [ -z "$PR_EXISTS" ]; then
            # Determine the default branch for PR base
            DEFAULT_BRANCH=$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)
            
            gh pr create \
              --title "Update Enclave EIF Version to ${{ steps.shorten-commit.outputs.sha_short }}" \
              --body "This PR updates the EIF version for the ${{ steps.env.outputs.environment }} environment.
              
          **Changes:**
          - EIF Version: \`${{ steps.shorten-commit.outputs.sha_short }}\`
          - S3 Path: \`s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif\`
          - Source Commit: ${{ github.sha }}
          
          **Next Steps:**
          1. Review the Terraform changes
          2. Merge this PR to apply the update
          3. Or run \`terraform apply\` manually in the environment directory" \
              --base "$DEFAULT_BRANCH"
            echo "‚úÖ Created PR for EIF version update"
          else
            echo "PR already exists: #$PR_EXISTS"
          fi

      - name: Apply Terraform changes
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üöÄ Applying Terraform changes..."
          
          # Initialize Terraform with backend config (without profile for OIDC)
          # Use -reconfigure to handle backend config changes without migration
          terraform init -reconfigure \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=$TF_BACKEND_DYNAMODB_TABLE"
          
          terraform plan \
            -var="eif_version=${{ steps.shorten-commit.outputs.sha_short }}"
          
          terraform apply -auto-approve \
            -var="eif_version=${{ steps.shorten-commit.outputs.sha_short }}"
          
          echo "‚úÖ Terraform applied successfully"
          
          # Get Auto Scaling Group name from Terraform output
          # The output name is autoscaling_group_name
          ASG_NAME=$(terraform output -raw autoscaling_group_name 2>/dev/null || echo "")
          if [ -z "$ASG_NAME" ]; then
            # Fallback: construct ASG name based on environment
            # Module name is "nautilus-watermark-{environment}", ASG suffix is "-asg"
            ASG_NAME="nautilus-watermark-${{ steps.env.outputs.environment }}-asg"
            echo "‚ö†Ô∏è  Could not get ASG name from terraform output, using fallback: $ASG_NAME"
          else
            echo "‚úÖ Got ASG name from terraform output: $ASG_NAME"
          fi
          echo "ASG_NAME=$ASG_NAME" >> $GITHUB_ENV

      - name: Ensure desired capacity is 1 before refresh
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üîß Ensuring desired capacity is 1 before instance refresh..."
          echo "This prevents instance refresh from creating extra instances"
          
          # Get current desired capacity
          CURRENT_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --query 'AutoScalingGroups[0].DesiredCapacity' \
            --output text 2>/dev/null || echo "1")
          
          echo "Current desired capacity: $CURRENT_CAPACITY"
          
          # Set to 1 if not already at 1
          if [ "$CURRENT_CAPACITY" != "1" ]; then
            echo "Setting desired capacity to 1..."
            aws autoscaling set-desired-capacity \
              --auto-scaling-group-name "$ASG_NAME" \
              --desired-capacity 1 \
              --honor-cooldown
            
            echo "Waiting for capacity to stabilize..."
            MAX_WAIT=300  # 5 minutes max wait
            ELAPSED=0
            CHECK_INTERVAL=10  # Check every 10 seconds
            
            while [ $ELAPSED -lt $MAX_WAIT ]; do
              ACTUAL_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names "$ASG_NAME" \
                --query 'AutoScalingGroups[0].DesiredCapacity' \
                --output text 2>/dev/null || echo "1")
              
              IN_SERVICE_COUNT=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names "$ASG_NAME" \
                --query "AutoScalingGroups[0].Instances[?LifecycleState=='InService'] | length(@)" \
                --output text 2>/dev/null || echo "0")
              
              echo "[$((ELAPSED/60))m ${ELAPSED}s] Desired: $ACTUAL_CAPACITY, In service: $IN_SERVICE_COUNT"
              
              if [ "$ACTUAL_CAPACITY" == "1" ] && [ "$IN_SERVICE_COUNT" -eq 1 ]; then
                echo "‚úÖ Desired capacity is 1 with 1 instance in service"
                break
              fi
              
              sleep $CHECK_INTERVAL
              ELAPSED=$((ELAPSED + CHECK_INTERVAL))
            done
            
            if [ $ELAPSED -ge $MAX_WAIT ]; then
              echo "‚ö†Ô∏è  Timeout waiting for capacity to stabilize, continuing anyway..."
            fi
          else
            echo "‚úÖ Already at desired capacity 1"
          fi

      - name: Trigger Auto Scaling Group Instance Refresh
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üîÑ Triggering Auto Scaling Group instance refresh..."
          echo "This will replace existing instances with new ones using the updated launch template"
          
          # Check if there's an existing instance refresh in progress
          EXISTING_REFRESH=$(aws autoscaling describe-instance-refreshes \
            --auto-scaling-group-name "$ASG_NAME" \
            --query 'InstanceRefreshes[?Status==`InProgress` || Status==`Pending`].InstanceRefreshId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_REFRESH" ]; then
            echo "‚ö†Ô∏è  Instance refresh already in progress: $EXISTING_REFRESH"
            echo "Waiting for existing refresh to complete or canceling it..."
            
            # Cancel existing refresh if it's been running for more than 30 minutes
            REFRESH_START_TIME=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$EXISTING_REFRESH" \
              --query 'InstanceRefreshes[0].StartTime' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$REFRESH_START_TIME" ]; then
              # Calculate time difference (simplified check)
              echo "Existing refresh started at: $REFRESH_START_TIME"
              # For simplicity, we'll cancel and start a new one
              aws autoscaling cancel-instance-refresh \
                --auto-scaling-group-name "$ASG_NAME" \
                --instance-refresh-id "$EXISTING_REFRESH" 2>/dev/null || true
              echo "Canceled existing refresh, starting new one..."
            fi
          fi
          
          # Start instance refresh with MinHealthyPercentage: 0 to prevent extra instances
          # Since we're at capacity 1, this ensures only 1 instance is replaced at a time
          REFRESH_ID=$(aws autoscaling start-instance-refresh \
            --auto-scaling-group-name "$ASG_NAME" \
            --preferences '{
              "MinHealthyPercentage": 0,
              "InstanceWarmup": 300,
              "CheckpointPercentages": [100],
              "CheckpointDelay": 60,
              "SkipMatching": false
            }' \
            --query 'InstanceRefreshId' \
            --output text)
          
          if [ -n "$REFRESH_ID" ] && [ "$REFRESH_ID" != "None" ]; then
            echo "‚úÖ Instance refresh started: $REFRESH_ID"
            echo "REFRESH_ID=$REFRESH_ID" >> $GITHUB_ENV
            
            # Wait a bit and check status
            echo "Waiting 30 seconds before checking status..."
            sleep 30
            
            REFRESH_STATUS=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$REFRESH_ID" \
              --query 'InstanceRefreshes[0].Status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "Instance refresh status: $REFRESH_STATUS"
            echo ""
            echo "üìä Instance refresh will:"
            echo "  1. Launch new instances with updated launch template"
            echo "  2. Wait for new instances to pass health checks"
            echo "  3. Terminate old instances"
            echo ""
            echo "You can monitor progress in AWS Console:"
            echo "  EC2 ‚Üí Auto Scaling Groups ‚Üí $ASG_NAME ‚Üí Instance refresh tab"
            echo ""
            echo "Or check status with:"
            echo "  aws autoscaling describe-instance-refreshes --auto-scaling-group-name $ASG_NAME"
          else
            echo "‚ö†Ô∏è  Failed to start instance refresh, but this is not critical"
            echo "Instances will be replaced when they are manually terminated or when health checks fail"
          fi

      - name: Wait for instance refresh to complete
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          if [ -z "${REFRESH_ID:-}" ] || [ "$REFRESH_ID" == "None" ] || [ "$REFRESH_ID" == "" ]; then
            echo "‚ö†Ô∏è  No instance refresh ID found, skipping wait"
            exit 0
          fi
          
          echo "‚è≥ Waiting for instance refresh to complete..."
          echo "This may take 10-15 minutes depending on instance startup and health checks"
          
          MAX_WAIT_TIME=1800  # 30 minutes max wait
          ELAPSED=0
          CHECK_INTERVAL=30  # Check every 30 seconds
          
          while [ $ELAPSED -lt $MAX_WAIT_TIME ]; do
            REFRESH_STATUS=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$REFRESH_ID" \
              --query 'InstanceRefreshes[0].Status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "[$((ELAPSED/60))m ${ELAPSED}s] Instance refresh status: $REFRESH_STATUS"
            
            if [ "$REFRESH_STATUS" == "Successful" ]; then
              echo "‚úÖ Instance refresh completed successfully!"
              break
            elif [ "$REFRESH_STATUS" == "Failed" ] || [ "$REFRESH_STATUS" == "Cancelled" ]; then
              echo "‚ö†Ô∏è  Instance refresh ended with status: $REFRESH_STATUS"
              echo "Continuing anyway - instances may still be updated"
              break
            fi
            
            sleep $CHECK_INTERVAL
            ELAPSED=$((ELAPSED + CHECK_INTERVAL))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT_TIME ]; then
            echo "‚ö†Ô∏è  Timeout waiting for instance refresh to complete"
            echo "Refresh may still be in progress. Check AWS Console for status."
          fi

      - name: Verify final instance count
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üîç Verifying final instance count..."
          
          # Get current desired capacity
          DESIRED_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --query 'AutoScalingGroups[0].DesiredCapacity' \
            --output text 2>/dev/null || echo "1")
          
          IN_SERVICE_COUNT=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --query "AutoScalingGroups[0].Instances[?LifecycleState=='InService'] | length(@)" \
            --output text 2>/dev/null || echo "0")
          
          echo "Desired capacity: $DESIRED_CAPACITY"
          echo "Instances in service: $IN_SERVICE_COUNT"
          
          # Ensure desired capacity is 1 (should already be 1, but double-check)
          if [ "$DESIRED_CAPACITY" != "1" ]; then
            echo "‚ö†Ô∏è  Desired capacity is not 1, setting to 1..."
            aws autoscaling set-desired-capacity \
              --auto-scaling-group-name "$ASG_NAME" \
              --desired-capacity 1 \
              --honor-cooldown
            echo "‚úÖ Set desired capacity to 1"
          else
            echo "‚úÖ Desired capacity is already 1"
          fi
          
          if [ "$IN_SERVICE_COUNT" -gt 1 ]; then
            echo "‚ö†Ô∏è  Warning: $IN_SERVICE_COUNT instances in service (expected 1)"
            echo "Extra instances will be terminated by Auto Scaling Group"
          else
            echo "‚úÖ Instance count is correct: $IN_SERVICE_COUNT"
          fi

      - name: Notify deployment status
        if: always()
        run: |
          if [ "${{ job.status }}" == 'success' ]; then
            echo "‚úÖ Deployment to ${{ steps.env.outputs.environment }} successful!"
          else
            echo "‚ùå Deployment to ${{ steps.env.outputs.environment }} failed!"
          fi

      - name: Deployment summary
        if: always()
        run: |
          echo "## üéâ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ steps.env.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Enclave App:** \`${{ env.ENCLAVE_APP }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**EIF Version:** \`${{ steps.shorten-commit.outputs.sha_short }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**S3 Path:** \`s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif\`" >> $GITHUB_STEP_SUMMARY
          echo "**Source Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add endpoint configuration info if available
          ALLOWLIST_PATH="src/nautilus-server/src/apps/${{ env.ENCLAVE_APP }}/allowed_endpoints.yaml"
          if [ -f "$ALLOWLIST_PATH" ]; then
            echo "**Allowed Endpoints:**" >> $GITHUB_STEP_SUMMARY
            if command -v yq >/dev/null 2>&1; then
              yq e '.endpoints[]' "$ALLOWLIST_PATH" | sed 's/^/  - /' >> $GITHUB_STEP_SUMMARY || echo "  (Unable to parse)" >> $GITHUB_STEP_SUMMARY
            else
              echo "  (Configured in \`$ALLOWLIST_PATH\`)" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ steps.check-eif.outputs.exists }}" == "true" ]; then
            echo "‚úÖ EIF file already existed in S3, skipped build" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚úÖ EIF file built and uploaded successfully" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ github.event.inputs.auto_apply }}" == "true" ]; then
            echo "‚úÖ Terraform changes applied automatically" >> $GITHUB_STEP_SUMMARY
            if [ -n "${REFRESH_ID:-}" ] && [ "$REFRESH_ID" != "None" ] && [ "$REFRESH_ID" != "" ]; then
              echo "üîÑ Instance refresh started: \`$REFRESH_ID\`" >> $GITHUB_STEP_SUMMARY
              echo "   Instances are being replaced with new launch template version" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ÑπÔ∏è  Instance refresh may be in progress (check AWS Console for status)" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ github.event_name }}" == "push" ] && [ "${{ steps.env.outputs.environment }}" == "staging" ]; then
            echo "‚úÖ Terraform changes applied automatically (staging auto-deploy)" >> $GITHUB_STEP_SUMMARY
            if [ -n "${REFRESH_ID:-}" ] && [ "$REFRESH_ID" != "None" ] && [ "$REFRESH_ID" != "" ]; then
              echo "üîÑ Instance refresh started: \`$REFRESH_ID\`" >> $GITHUB_STEP_SUMMARY
              echo "   Instances are being replaced with new launch template version" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ÑπÔ∏è  Instance refresh may be in progress (check AWS Console for status)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "üìù Pull Request created for Terraform changes" >> $GITHUB_STEP_SUMMARY
            echo "   Review and merge to apply the update" >> $GITHUB_STEP_SUMMARY
            echo "   After merging, instance refresh will be triggered automatically" >> $GITHUB_STEP_SUMMARY
          fi

