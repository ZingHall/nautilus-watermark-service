name: Deploy to Nitro Enclave

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'Containerfile'
      - 'Makefile'
      - 'Cargo.toml'
      - '.github/workflows/deploy-enclave.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      auto_apply:
        description: 'Automatically apply Terraform changes'
        required: false
        default: false
        type: boolean

env:
  AWS_ACCOUNT_ID: 287767576800
  AWS_REGION: ap-northeast-1
  ENCLAVE_APP: zing-watermark
  S3_BUCKET_STAGING: zing-enclave-artifacts-staging
  S3_BUCKET_PRODUCTION: zing-enclave-artifacts-production
  EIF_PATH_STAGING: eif/staging
  EIF_PATH_PRODUCTION: eif/production

jobs:
  deploy:
    name: Deploy to Nitro Enclave
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine environment
        id: env
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            echo "environment=staging" >> $GITHUB_OUTPUT
          fi
          echo "Deploying to: ${{ steps.env.outputs.environment }}"

      - name: Shorten commit hash
        id: shorten-commit
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Set S3 bucket and path
        id: s3
        run: |
          if [ "${{ steps.env.outputs.environment }}" == "production" ]; then
            echo "bucket=${{ env.S3_BUCKET_PRODUCTION }}" >> $GITHUB_OUTPUT
            echo "path=${{ env.EIF_PATH_PRODUCTION }}" >> $GITHUB_OUTPUT
          else
            echo "bucket=${{ env.S3_BUCKET_STAGING }}" >> $GITHUB_OUTPUT
            echo "path=${{ env.EIF_PATH_STAGING }}" >> $GITHUB_OUTPUT
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/github-actions-cicd-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Checkout infrastructure repository
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/zing-infra
          path: zing-infra
          token: ${{ secrets.ZING_INFRA_TOKEN }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
          terraform_wrapper: false

      - name: Force unlock Terraform state (if locked)
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üîì Checking for stuck Terraform state lock..."
          
          # Initialize Terraform with backend config
          terraform init -reconfigure \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=$TF_BACKEND_DYNAMODB_TABLE"
          
          # Try to run a plan to check if state is locked
          set +e  # Don't exit on error
          terraform plan -detailed-exitcode 2>&1 | tee plan_output.txt
          PLAN_EXIT_CODE=$?
          set -e
          
          # Check if the plan failed due to a lock
          if grep -q "Error acquiring the state lock" plan_output.txt; then
            echo "‚ö†Ô∏è Terraform state is locked, attempting to extract lock ID..."
            
            # Extract lock ID from error message
            LOCK_ID=$(grep -oP 'ID:\s+\K[a-f0-9-]+' plan_output.txt | head -1)
            
            if [ -n "$LOCK_ID" ]; then
              echo "Found lock ID: $LOCK_ID"
              echo "Attempting to force unlock..."
              terraform force-unlock -force "$LOCK_ID"
              echo "‚úÖ Successfully unlocked Terraform state"
            else
              echo "‚ùå Could not extract lock ID from error message"
              echo "Lock may need to be manually removed"
              exit 1
            fi
          else
            echo "‚úÖ Terraform state is not locked, proceeding..."
          fi
          
          rm -f plan_output.txt

      - name: Verify Terraform permissions (plan only)
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üîç Verifying Terraform permissions with terraform plan..."
          echo "This step validates that the IAM role has all required permissions"
          echo "before building EIF files or making any changes."
          
          # Run terraform plan to verify permissions
          # This will fail if any required permissions are missing
          # Use current eif_version from variables.tf for this check
          set +e  # Don't exit on error, we'll handle it manually
          terraform plan -detailed-exitcode
          PLAN_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          if [ "$PLAN_EXIT_CODE" = "1" ]; then
            echo "‚ùå terraform plan failed - check IAM permissions"
            echo ""
            echo "The IAM role 'github-actions-cicd-role' is missing required permissions."
            echo ""
            echo "Current policy includes (with wildcards):"
            echo "  - s3:*, dynamodb:*, ec2:*, vpc:*, rds:*"
            echo "  - route53:*, acm:*, elasticloadbalancing:*"
            echo "  - logs:*, cloudwatch:*, autoscaling:*"
            echo "  - Limited IAM permissions (roles, instance profiles)"
            echo ""
            echo "Check the error message above to identify the specific missing permission."
            echo "Then update zing-infra/modules/aws/github-cicd/main.tf to add it."
            echo ""
            echo "If it's a new service, you may need to add a wildcard (e.g., 'servicename:*')"
            echo "or specific permissions to the terraform_policy."
            exit 1
          elif [ "$PLAN_EXIT_CODE" = "2" ]; then
            echo "‚úÖ terraform plan succeeded - changes detected (this is expected)"
            echo "All required permissions are available"
          elif [ "$PLAN_EXIT_CODE" = "0" ]; then
            echo "‚úÖ terraform plan succeeded - no changes needed"
            echo "All required permissions are available"
          else
            echo "‚ö†Ô∏è terraform plan returned unexpected exit code: $PLAN_EXIT_CODE"
            echo "This might indicate permission issues. Please check the output above."
            exit 1
          fi

      - name: Check if EIF already exists in S3
        id: check-eif
        run: |
          EIF_S3_PATH="s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif"
          if aws s3 ls "$EIF_S3_PATH" 2>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ EIF file already exists in S3, skipping build"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è EIF file not found, will build new one"
          fi

      - name: Build EIF file
        if: steps.check-eif.outputs.exists == 'false'
        run: |
          echo "üî® Building EIF file..."
          make ENCLAVE_APP=${{ env.ENCLAVE_APP }}
          
          if [ ! -f "out/nitro.eif" ]; then
            echo "‚ùå EIF file not found after build"
            exit 1
          fi
          
          echo "‚úÖ EIF file built successfully"
          ls -lh out/nitro.eif

      - name: Upload EIF to S3
        if: steps.check-eif.outputs.exists == 'false'
        id: upload-eif
        run: |
          EIF_S3_PATH="s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif"
          echo "üì§ Uploading EIF to $EIF_S3_PATH"
          
          aws s3 cp out/nitro.eif "$EIF_S3_PATH" \
            --metadata "commit=${{ github.sha }},commit_short=${{ steps.shorten-commit.outputs.sha_short }},build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          
          echo "eif_path=$EIF_S3_PATH" >> $GITHUB_OUTPUT
          echo "‚úÖ EIF uploaded successfully"

      - name: Update Terraform EIF version
        id: update-eif-version
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        run: |
          echo "üìù Updating Terraform configuration..."
          
          # Update eif_version default value in variables.tf using Python
          python3 << 'PYTHON_SCRIPT'
          import re
          
          with open('variables.tf', 'r') as f:
              content = f.read()
          
          # Pattern to match eif_version default value in variables.tf
          pattern = r'(variable\s+"eif_version"\s*\{[^}]*default\s*=\s*)"[^"]*"'
          replacement = r'\1"${{ steps.shorten-commit.outputs.sha_short }}"'
          
          UPDATED_FILE = None
          
          if re.search(pattern, content):
              # Update existing eif_version default value
              content = re.sub(pattern, replacement, content)
              with open('variables.tf', 'w') as f:
                  f.write(content)
              UPDATED_FILE = 'variables.tf'
          else:
              # Fallback: try to find and update in main.tf (for backwards compatibility)
              with open('main.tf', 'r') as f:
                  main_content = f.read()
              
              main_pattern = r'eif_version\s*=\s*"[^"]*"'
              main_replacement = f'eif_version    = "${{ steps.shorten-commit.outputs.sha_short }}"'
              
              if re.search(main_pattern, main_content):
                  main_content = re.sub(main_pattern, main_replacement, main_content)
                  with open('main.tf', 'w') as f:
                      f.write(main_content)
                  UPDATED_FILE = 'main.tf'
          
          if UPDATED_FILE:
              print(f"Updated eif_version in {UPDATED_FILE}")
          else:
              print("ERROR: Could not find eif_version to update")
              exit(1)
          PYTHON_SCRIPT
          
          echo "‚úÖ Updated eif_version default to ${{ steps.shorten-commit.outputs.sha_short }}"
          echo "Changes:"
          git diff variables.tf || git diff main.tf || true
          
          # Determine which file was updated and set as output for next step
          if git diff --name-only | grep -q variables.tf; then
            echo "updated_file=variables.tf" >> $GITHUB_OUTPUT
          elif git diff --name-only | grep -q main.tf; then
            echo "updated_file=main.tf" >> $GITHUB_OUTPUT
          else
            echo "ERROR: No changes detected in variables.tf or main.tf"
            exit 1
          fi

      - name: Commit Terraform changes
        id: commit-changes
        working-directory: zing-infra
        env:
          GITHUB_TOKEN: ${{ secrets.ZING_INFRA_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Determine if we should create a PR or push directly to main
          CREATE_PR=false
          if [ "${{ github.event_name }}" == "push" ] && [ "${{ github.ref }}" == "refs/heads/main" ] && [ "${{ steps.env.outputs.environment }}" == "production" ]; then
            CREATE_PR=true
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ github.event.inputs.auto_apply }}" != "true" ]; then
            CREATE_PR=true
          fi
          
          # Fetch latest changes from remote
          git fetch origin
          
          # Determine the default branch (master or main)
          DEFAULT_BRANCH=$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)
          echo "Default branch is: $DEFAULT_BRANCH"
          
          # Create branch if we need a PR, otherwise work on default branch
          if [ "$CREATE_PR" == "true" ]; then
            BRANCH_NAME="update-enclave-eif-${{ steps.shorten-commit.outputs.sha_short }}"
            git checkout -b "$BRANCH_NAME" "origin/$DEFAULT_BRANCH"
            echo "Created branch: $BRANCH_NAME"
          else
            git checkout "$DEFAULT_BRANCH" || git checkout -b "$DEFAULT_BRANCH"
            git pull origin "$DEFAULT_BRANCH" --rebase || true
            echo "Working on $DEFAULT_BRANCH branch"
          fi
          
          # Get the updated file from previous step
          UPDATED_FILE="${{ steps.update-eif-version.outputs.updated_file }}"
          if [ -z "$UPDATED_FILE" ]; then
            echo "ERROR: Could not determine which file was updated"
            exit 1
          fi
          
          echo "Updating file: $UPDATED_FILE"
          
          git add "environments/${{ steps.env.outputs.environment }}/nautilus-enclave/$UPDATED_FILE"
          
          # Check if there are any changes to commit
          if git diff --staged --quiet; then
            echo "‚ö†Ô∏è No changes to commit (file may already be up to date)"
            echo "skip_commit=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "skip_commit=false" >> $GITHUB_OUTPUT
          
          COMMIT_MSG="chore: update enclave EIF version to ${{ steps.shorten-commit.outputs.sha_short }}

          - EIF file: s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif
          - Source commit: ${{ github.sha }}
          - Environment: ${{ steps.env.outputs.environment }}"
          
          git commit -m "$COMMIT_MSG"
          echo "‚úÖ Committed changes to $UPDATED_FILE"
          
          # Push the branch
          if [ "$CREATE_PR" == "true" ]; then
            git push origin "$BRANCH_NAME"
            echo "Pushed branch: $BRANCH_NAME"
          else
            git push origin "$DEFAULT_BRANCH"
            echo "Pushed to $DEFAULT_BRANCH branch"
          fi

      - name: Create Pull Request for Terraform changes
        if: |
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'production') ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply != 'true')
        working-directory: zing-infra
        env:
          GITHUB_TOKEN: ${{ secrets.ZING_INFRA_TOKEN }}
        run: |
          # Skip if commit was skipped
          if [ "${{ steps.commit-changes.outputs.skip_commit }}" == "true" ]; then
            echo "‚ö†Ô∏è Skipping PR creation (no changes to commit)"
            exit 0
          fi
          
          BRANCH_NAME="update-enclave-eif-${{ steps.shorten-commit.outputs.sha_short }}"
          
          # Check if PR already exists
          PR_EXISTS=$(gh pr list --head "$BRANCH_NAME" --json number --jq '.[0].number' 2>/dev/null || echo "")
          
          if [ -z "$PR_EXISTS" ]; then
            # Determine the default branch for PR base
            DEFAULT_BRANCH=$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)
            
            gh pr create \
              --title "Update Enclave EIF Version to ${{ steps.shorten-commit.outputs.sha_short }}" \
              --body "This PR updates the EIF version for the ${{ steps.env.outputs.environment }} environment.
              
          **Changes:**
          - EIF Version: \`${{ steps.shorten-commit.outputs.sha_short }}\`
          - S3 Path: \`s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif\`
          - Source Commit: ${{ github.sha }}
          
          **Next Steps:**
          1. Review the Terraform changes
          2. Merge this PR to apply the update
          3. Or run \`terraform apply\` manually in the environment directory" \
              --base "$DEFAULT_BRANCH"
            echo "‚úÖ Created PR for EIF version update"
          else
            echo "PR already exists: #$PR_EXISTS"
          fi

      - name: Apply Terraform changes
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        working-directory: zing-infra/environments/${{ steps.env.outputs.environment }}/nautilus-enclave
        env:
          TF_BACKEND_BUCKET: terraform-zing-${{ steps.env.outputs.environment }}
          TF_BACKEND_KEY: nautilus-enclave.tfstate
          TF_BACKEND_REGION: ${{ env.AWS_REGION }}
          TF_BACKEND_DYNAMODB_TABLE: terraform-lock-table
        run: |
          echo "üöÄ Applying Terraform changes..."
          
          # Initialize Terraform with backend config (without profile for OIDC)
          # Use -reconfigure to handle backend config changes without migration
          terraform init -reconfigure \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=$TF_BACKEND_DYNAMODB_TABLE"
          
          terraform plan \
            -var="eif_version=${{ steps.shorten-commit.outputs.sha_short }}"
          
          terraform apply -auto-approve \
            -var="eif_version=${{ steps.shorten-commit.outputs.sha_short }}"
          
          echo "‚úÖ Terraform applied successfully"
          
          # Get Auto Scaling Group name from Terraform output
          # The output name is autoscaling_group_name
          ASG_NAME=$(terraform output -raw autoscaling_group_name 2>/dev/null || echo "")
          if [ -z "$ASG_NAME" ]; then
            # Fallback: construct ASG name based on environment
            # Module name is "nautilus-watermark-{environment}", ASG suffix is "-asg"
            ASG_NAME="nautilus-watermark-${{ steps.env.outputs.environment }}-asg"
            echo "‚ö†Ô∏è  Could not get ASG name from terraform output, using fallback: $ASG_NAME"
          else
            echo "‚úÖ Got ASG name from terraform output: $ASG_NAME"
          fi
          echo "ASG_NAME=$ASG_NAME" >> $GITHUB_ENV

      - name: Scale up for zero-downtime deployment
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üìà Scaling up to 2 instances for zero-downtime deployment..."
          
          # Get current desired capacity
          CURRENT_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --query 'AutoScalingGroups[0].DesiredCapacity' \
            --output text 2>/dev/null || echo "1")
          
          echo "Current desired capacity: $CURRENT_CAPACITY"
          
          # Scale up to 2 instances if not already at 2
          if [ "$CURRENT_CAPACITY" != "2" ]; then
            echo "Scaling up to 2 instances..."
            aws autoscaling set-desired-capacity \
              --auto-scaling-group-name "$ASG_NAME" \
              --desired-capacity 2 \
              --honor-cooldown
            
            echo "Waiting for instances to be in service..."
            # Wait for instances to be in service using polling
            MAX_WAIT=300  # 5 minutes max wait
            ELAPSED=0
            CHECK_INTERVAL=10  # Check every 10 seconds
            
            while [ $ELAPSED -lt $MAX_WAIT ]; do
              IN_SERVICE_COUNT=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names "$ASG_NAME" \
                --query "AutoScalingGroups[0].Instances[?LifecycleState=='InService'] | length(@)" \
                --output text 2>/dev/null || echo "0")
              
              DESIRED_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names "$ASG_NAME" \
                --query 'AutoScalingGroups[0].DesiredCapacity' \
                --output text 2>/dev/null || echo "0")
              
              echo "[$((ELAPSED/60))m ${ELAPSED}s] Instances in service: $IN_SERVICE_COUNT / $DESIRED_CAPACITY"
              
              if [ "$IN_SERVICE_COUNT" -ge "$DESIRED_CAPACITY" ] && [ "$IN_SERVICE_COUNT" -gt 0 ]; then
                echo "‚úÖ All instances are in service"
                break
              fi
              
              sleep $CHECK_INTERVAL
              ELAPSED=$((ELAPSED + CHECK_INTERVAL))
            done
            
            if [ $ELAPSED -ge $MAX_WAIT ]; then
              echo "‚ö†Ô∏è  Timeout waiting for instances to be in service, continuing anyway..."
            fi
            
            # Give instances a bit more time to fully initialize
            echo "Waiting 60 seconds for instances to stabilize..."
            sleep 60
            echo "‚úÖ Scaled up to 2 instances"
          else
            echo "‚úÖ Already at 2 instances, no scaling needed"
          fi

      - name: Trigger Auto Scaling Group Instance Refresh
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üîÑ Triggering Auto Scaling Group instance refresh..."
          echo "This will replace existing instances with new ones using the updated launch template"
          
          # Check if there's an existing instance refresh in progress
          EXISTING_REFRESH=$(aws autoscaling describe-instance-refreshes \
            --auto-scaling-group-name "$ASG_NAME" \
            --query 'InstanceRefreshes[?Status==`InProgress` || Status==`Pending`].InstanceRefreshId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_REFRESH" ]; then
            echo "‚ö†Ô∏è  Instance refresh already in progress: $EXISTING_REFRESH"
            echo "Waiting for existing refresh to complete or canceling it..."
            
            # Cancel existing refresh if it's been running for more than 30 minutes
            REFRESH_START_TIME=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$EXISTING_REFRESH" \
              --query 'InstanceRefreshes[0].StartTime' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$REFRESH_START_TIME" ]; then
              # Calculate time difference (simplified check)
              echo "Existing refresh started at: $REFRESH_START_TIME"
              # For simplicity, we'll cancel and start a new one
              aws autoscaling cancel-instance-refresh \
                --auto-scaling-group-name "$ASG_NAME" \
                --instance-refresh-id "$EXISTING_REFRESH" 2>/dev/null || true
              echo "Canceled existing refresh, starting new one..."
            fi
          fi
          
          # Start instance refresh
          REFRESH_ID=$(aws autoscaling start-instance-refresh \
            --auto-scaling-group-name "$ASG_NAME" \
            --preferences '{
              "MinHealthyPercentage": 50,
              "InstanceWarmup": 300,
              "CheckpointPercentages": [50, 100],
              "CheckpointDelay": 60,
              "SkipMatching": false
            }' \
            --query 'InstanceRefreshId' \
            --output text)
          
          if [ -n "$REFRESH_ID" ] && [ "$REFRESH_ID" != "None" ]; then
            echo "‚úÖ Instance refresh started: $REFRESH_ID"
            echo "REFRESH_ID=$REFRESH_ID" >> $GITHUB_ENV
            
            # Wait a bit and check status
            echo "Waiting 30 seconds before checking status..."
            sleep 30
            
            REFRESH_STATUS=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$REFRESH_ID" \
              --query 'InstanceRefreshes[0].Status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "Instance refresh status: $REFRESH_STATUS"
            echo ""
            echo "üìä Instance refresh will:"
            echo "  1. Launch new instances with updated launch template"
            echo "  2. Wait for new instances to pass health checks"
            echo "  3. Terminate old instances"
            echo ""
            echo "You can monitor progress in AWS Console:"
            echo "  EC2 ‚Üí Auto Scaling Groups ‚Üí $ASG_NAME ‚Üí Instance refresh tab"
            echo ""
            echo "Or check status with:"
            echo "  aws autoscaling describe-instance-refreshes --auto-scaling-group-name $ASG_NAME"
          else
            echo "‚ö†Ô∏è  Failed to start instance refresh, but this is not critical"
            echo "Instances will be replaced when they are manually terminated or when health checks fail"
          fi

      - name: Wait for instance refresh to complete
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          if [ -z "${REFRESH_ID:-}" ] || [ "$REFRESH_ID" == "None" ] || [ "$REFRESH_ID" == "" ]; then
            echo "‚ö†Ô∏è  No instance refresh ID found, skipping wait"
            exit 0
          fi
          
          echo "‚è≥ Waiting for instance refresh to complete..."
          echo "This may take 10-15 minutes depending on instance startup and health checks"
          
          MAX_WAIT_TIME=1800  # 30 minutes max wait
          ELAPSED=0
          CHECK_INTERVAL=30  # Check every 30 seconds
          
          while [ $ELAPSED -lt $MAX_WAIT_TIME ]; do
            REFRESH_STATUS=$(aws autoscaling describe-instance-refreshes \
              --auto-scaling-group-name "$ASG_NAME" \
              --instance-refresh-ids "$REFRESH_ID" \
              --query 'InstanceRefreshes[0].Status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "[$((ELAPSED/60))m ${ELAPSED}s] Instance refresh status: $REFRESH_STATUS"
            
            if [ "$REFRESH_STATUS" == "Successful" ]; then
              echo "‚úÖ Instance refresh completed successfully!"
              break
            elif [ "$REFRESH_STATUS" == "Failed" ] || [ "$REFRESH_STATUS" == "Cancelled" ]; then
              echo "‚ö†Ô∏è  Instance refresh ended with status: $REFRESH_STATUS"
              echo "Continuing anyway - instances may still be updated"
              break
            fi
            
            sleep $CHECK_INTERVAL
            ELAPSED=$((ELAPSED + CHECK_INTERVAL))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT_TIME ]; then
            echo "‚ö†Ô∏è  Timeout waiting for instance refresh to complete"
            echo "Refresh may still be in progress. Check AWS Console for status."
          fi

      - name: Scale down to 1 instance after deployment
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.auto_apply == 'true') ||
          (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.env.outputs.environment == 'staging')
        run: |
          echo "üìâ Scaling down to 1 instance after successful deployment..."
          
          # Scale down to 1 instance
          aws autoscaling set-desired-capacity \
            --auto-scaling-group-name "$ASG_NAME" \
            --desired-capacity 1 \
            --honor-cooldown
          
          echo "‚úÖ Scaled down to 1 instance"
          echo "The Auto Scaling Group will terminate one instance, keeping the newly deployed one"

      - name: Notify deployment status
        if: always()
        run: |
          if [ "${{ job.status }}" == 'success' ]; then
            echo "‚úÖ Deployment to ${{ steps.env.outputs.environment }} successful!"
          else
            echo "‚ùå Deployment to ${{ steps.env.outputs.environment }} failed!"
          fi

      - name: Deployment summary
        if: always()
        run: |
          echo "## üéâ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ steps.env.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**EIF Version:** \`${{ steps.shorten-commit.outputs.sha_short }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**S3 Path:** \`s3://${{ steps.s3.outputs.bucket }}/${{ steps.s3.outputs.path }}/nitro-${{ steps.shorten-commit.outputs.sha_short }}.eif\`" >> $GITHUB_STEP_SUMMARY
          echo "**Source Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.check-eif.outputs.exists }}" == "true" ]; then
            echo "‚úÖ EIF file already existed in S3, skipped build" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚úÖ EIF file built and uploaded successfully" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ github.event.inputs.auto_apply }}" == "true" ]; then
            echo "‚úÖ Terraform changes applied automatically" >> $GITHUB_STEP_SUMMARY
            if [ -n "${REFRESH_ID:-}" ] && [ "$REFRESH_ID" != "None" ] && [ "$REFRESH_ID" != "" ]; then
              echo "üîÑ Instance refresh started: \`$REFRESH_ID\`" >> $GITHUB_STEP_SUMMARY
              echo "   Instances are being replaced with new launch template version" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ÑπÔ∏è  Instance refresh may be in progress (check AWS Console for status)" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ github.event_name }}" == "push" ] && [ "${{ steps.env.outputs.environment }}" == "staging" ]; then
            echo "‚úÖ Terraform changes applied automatically (staging auto-deploy)" >> $GITHUB_STEP_SUMMARY
            if [ -n "${REFRESH_ID:-}" ] && [ "$REFRESH_ID" != "None" ] && [ "$REFRESH_ID" != "" ]; then
              echo "üîÑ Instance refresh started: \`$REFRESH_ID\`" >> $GITHUB_STEP_SUMMARY
              echo "   Instances are being replaced with new launch template version" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ÑπÔ∏è  Instance refresh may be in progress (check AWS Console for status)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "üìù Pull Request created for Terraform changes" >> $GITHUB_STEP_SUMMARY
            echo "   Review and merge to apply the update" >> $GITHUB_STEP_SUMMARY
            echo "   After merging, instance refresh will be triggered automatically" >> $GITHUB_STEP_SUMMARY
          fi

